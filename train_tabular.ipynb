{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import TpotAutoml\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user inputs:\n",
    "SEED = 13\n",
    "np.random.seed(SEED)\n",
    "KEEP_ROWS_FRAC = 1.0 #0.05 # set to 1 if all rows are meant to be kept\n",
    "skiprows_func = lambda i: i>0 and np.random.rand() > KEEP_ROWS_FRAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols=['item_id', 'activation_date', 'city', 'region',\n",
    "                              'parent_category_name', 'category_name', \n",
    "                             'item_seq_number', 'user_type', 'price_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = pd.read_csv('data/all_samples_no_nulls.csv', usecols=usecols, skiprows=skiprows_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', usecols=['item_id', 'activation_date', 'deal_probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(all_samples, how='left', on=['item_id', 'activation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(by='activation_date').drop('activation_date', axis=1).set_index('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'deal_probability'\n",
    "TIMEOUT_MINS = None\n",
    "SCORING = 'r2'\n",
    "X = (train.drop(target, axis=1)).values\n",
    "y = train[target].values\n",
    "tss = TimeSeriesSplit(n_splits=4)\n",
    "train_index, test_index = list(tss.split(X))[-1]\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "config_dict = {'sklearn.ensemble.GradientBoostingRegressor': {\n",
    "        'n_estimators': [100, 200, 400],\n",
    "        'loss': [\"ls\", \"lad\", \"huber\", \"quantile\"],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'max_depth': range(1, 11),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'alpha': [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "    },\n",
    "        'sklearn.ensemble.RandomForestRegressor': {\n",
    "        'n_estimators': [100, 200, 400],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "              }\n",
    "\n",
    "tpot = TpotAutoml(mode='regression',\n",
    "                  max_time_mins=TIMEOUT_MINS,\n",
    "                  generations = 1, population_size=1,\n",
    "                  scoring=SCORING,\n",
    "                  random_state=SEED,\n",
    "                  n_jobs=1,\n",
    "                  verbosity=2,\n",
    "                  cv=TimeSeriesSplit(n_splits=3),\n",
    "                  config_dict=config_dict,                 \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = GradientBoostingRegressor()\n",
    "base_model = RandomForestRegressor()\n",
    "# tpot = RandomizedSearchCV(estimator=base_model, random_state=RS,\n",
    "# #                           param_distributions=config_dict['sklearn.ensemble.GradientBoostingRegressor'],\n",
    "#                           param_distributions=config_dict['sklearn.ensemble.RandomForestRegressor'],\n",
    "#                          n_iter=15,\n",
    "#                          scoring='r2',\n",
    "#                          cv=TimeSeriesSplit(n_splits=4),\n",
    "#                          verbose=2,\n",
    "#                          n_jobs=4)\n",
    "\n",
    "tpot = RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
    "           max_features=0.2, max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=20, min_samples_split=4,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=6,\n",
    "           oob_score=False, random_state=None, verbose=1, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_importance_entropy = list(zip(train.drop(target, axis=1).columns.values, tpot.best_estimator_.feature_importances_))\n",
    "feat_importance_entropy = list(zip(train.drop(target, axis=1).columns.values, tpot.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(feat_importance_entropy), key=lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = tpot.score(X_test, y_test)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean((tpot.predict(X_test) - y_test) ** 2))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tpot.predict(X_test), log=True, bins=100)\n",
    "print(tpot.predict(X_test).mean())\n",
    "print(y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import Analysis\n",
    "ea = Analysis(tpot, X_train, y_train, X_test, y_test,\n",
    "                           mode='regression', target=target,\n",
    "                           features=train.drop(target, axis=1).columns,\n",
    "                           test_samples_index=test_index, random_state=RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ea.get_feature_importance(sort=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ea.feature_importance.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv', usecols=['item_id', 'activation_date'])\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.merge(all_samples, how='left', on=['item_id', 'activation_date'])\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.sort_values(by='activation_date').drop('activation_date', axis=1).set_index('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.get_dummies(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['deal_probability'] = tpot.predict(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_name = 'rf_tabular'\n",
    "pickle.dump(tpot, open('predictions/{}.pickle'.format(rf_tabular), 'wb'))\n",
    "model = pickle.load(open('predictions/{}.pickle'.format(rf_tabular), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = model.score(X_test, y_test)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['deal_probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
